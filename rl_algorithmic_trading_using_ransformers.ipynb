{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Transformer model\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_layers=2, hidden_size=32, num_heads=2, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.transformer = nn.TransformerEncoderLayer(d_model=input_size, nhead=num_heads, dim_feedforward=hidden_size, dropout=dropout)\n",
    "        self.encoder = nn.TransformerEncoder(self.transformer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = x.mean(dim=1)  # Average across the time steps\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the RL trading agent\n",
    "class RLTrader:\n",
    "    def __init__(self, input_size, output_size, learning_rate=0.001, gamma=0.95):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # Create the transformer model\n",
    "        self.model = TransformerModel(input_size, output_size)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "\n",
    "    def predict(self, state):\n",
    "        state = torch.tensor(state, dtype=torch.float32)\n",
    "        with torch.no_grad():\n",
    "            prediction = self.model(state)\n",
    "        return prediction.numpy()[0]\n",
    "\n",
    "    def train(self, states, actions, rewards):\n",
    "        states = torch.tensor(states, dtype=torch.float32)\n",
    "        actions = torch.tensor(actions, dtype=torch.long)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "\n",
    "        # Compute predicted Q values\n",
    "        predicted_values = self.model(states)\n",
    "        predicted_q_values = predicted_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        # Compute target Q values using temporal difference target\n",
    "        target_q_values = rewards\n",
    "\n",
    "        # Compute the loss (mean squared error between predicted and target Q values)\n",
    "        loss = nn.MSELoss()(predicted_q_values, target_q_values)\n",
    "\n",
    "        # Update the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform a trading simulation\n",
    "def run_simulation(prices, trader, initial_balance=1000, num_days=100, max_stock=5, epsilon=0.1):\n",
    "    balance = initial_balance\n",
    "    stock = 0\n",
    "    history = []\n",
    "\n",
    "    for day in range(num_days):\n",
    "        if day > 60:  # We need at least 60 days of history to make predictions\n",
    "            state = prices[day-60:day].reshape(1, 1, 60)  # Convert the history into a 3D tensor\n",
    "            action_values = trader.predict(state)\n",
    "\n",
    "            # Epsilon-greedy exploration\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = np.random.choice(trader.output_size)\n",
    "            else:\n",
    "                action = np.argmax(action_values)\n",
    "\n",
    "            # Perform the chosen action and update balance and stock\n",
    "            if action == 0:  # Buy\n",
    "                if balance >= prices[day] and stock < max_stock:\n",
    "                    stock_to_buy = min(int(balance / prices[day]), max_stock - stock)\n",
    "                    stock += stock_to_buy\n",
    "                    balance -= stock_to_buy * prices[day]\n",
    "            elif action == 1:  # Sell\n",
    "                if stock > 0:\n",
    "                    balance += stock * prices[day]\n",
    "                    stock = 0\n",
    "\n",
    "            # Calculate daily returns\n",
    "            daily_returns = (balance + stock * prices[day]) - initial_balance\n",
    "            history.append((state, action, daily_returns))\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to run the algorithmic trading and training\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate some random price data for demonstration\n",
    "    np.random.seed(42)\n",
    "    prices = np.random.rand(200) * 100  # Assuming the prices are in the range [0, 100]\n",
    "\n",
    "    # Define the input size and output size for the RLTrader\n",
    "    input_size = 60  # History of last 60 days\n",
    "    output_size = 3  # Buy, Sell, Hold\n",
    "\n",
    "    # Initialize the RLTrader\n",
    "    trader = RLTrader(input_size, output_size)\n",
    "\n",
    "    # Training parameters\n",
    "    num_episodes = 1000\n",
    "    batch_size = 32\n",
    "    epsilon = 0.1\n",
    "    gamma = 0.95\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        history = run_simulation(prices, trader, epsilon=epsilon)\n",
    "        states, actions, returns = zip(*history)\n",
    "\n",
    "        # Compute discounted returns\n",
    "        discounted_returns = []\n",
    "        G = 0\n",
    "        for r in reversed(returns):\n",
    "            G = r + gamma * G\n",
    "            discounted_returns.insert(0, G)\n",
    "\n",
    "        # Normalize discounted returns\n",
    "        discounted_returns = np.array(discounted_returns)\n",
    "        discounted_returns = (discounted_returns - np.mean(discounted_returns)) / (np.std(discounted_returns) + 1e-9)\n",
    "\n",
    "        # Convert lists to numpy arrays\n",
    "        states = np.array(states).squeeze(1)\n",
    "        actions = np.array(actions)\n",
    "\n",
    "        # Create DataLoader for training\n",
    "        dataset = TensorDataset(torch.tensor(states, dtype=torch.float32), torch.tensor(actions, dtype=torch.long), torch.tensor(discounted_returns, dtype=torch.float32))\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        # Train the trader on the current episode's data\n",
    "        for batch_states, batch_actions, batch_returns in dataloader:\n",
    "            trader.train(batch_states, batch_actions, batch_returns)\n",
    "\n",
    "        # Print progress\n",
    "        if (episode + 1) % 100 == 0:\n",
    "            print(f\"Episode {episode + 1}/{num_episodes}\")\n",
    "\n",
    "    # Evaluate the trained agent on a test set\n",
    "    test_prices = np.random.rand(100) * 100\n",
    "    test_history = run_simulation(test_prices, trader)\n",
    "    test_states, test_actions, test_returns = zip(*test_history)\n",
    "    print(f\"Final balance: ${np.mean(test_returns):.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
